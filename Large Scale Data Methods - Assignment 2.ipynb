{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Pool\n",
    "import pycorenlp\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Majority of code for this assignment located in pdfscrape and cityscrape folders\n",
    "from pdfscrape import pdf_pipeline_ns\n",
    "from cityscrape import scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSDM Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "\n",
    "PDF documents on municipal government websites hold lots of information, though are also used for paper-based processes that allow residents and constituents to download and print the PDF form. This project is the beginning of parrellizing the process of identifying, scraping, and eventually classifying PDF documents in order to find the PDF forms in what is often a large haystack of PDFs. For this project, who main components have been developed:  \n",
    "- Parrallelized web crawler to collect PDF URLs\n",
    "- Parrellelized PDF scraper which also hits a Stanford CoreNLP Service running in an AWS EC2 instance\n",
    "\n",
    "### Parrellized Web Crawler\n",
    "The web crawler starts crawling from a single web page, collects all URLs on the page, and puts them into a queue. The next URL is then pulled from the queue, and again, each of the URLs on that second page get put into the queue. If allowed to complete, all of the URLs in the queue would be visited on a First In First Out basis. The web crawler has a few key features:\n",
    "- limiting domain: Ability to limit visiting and scraping of a web page to a given domain\n",
    "- limiting path: Ability to limit visiting and scraping of a web page to any URL with a given string in the web address (example: only visit URLs with '/dept/finance/' in the URL).\n",
    "- number of pages: Limit the number of pages to be visited and scraped. The full scope of URLs collected can easily grow out of control.\n",
    "\n",
    "See source code: https://github.com/vidal-anguiano/LSDM-Assignment-2/blob/master/cityscrape/scrape.py\n",
    "\n",
    "### Parrellized PDF Scraper\n",
    "The parallelized webscraper takes PDF URLs from the shared mp.Queue, pdflink_q (see below) as soon as one is made available by the web crawller. The PDF scraper can take several parameters which allow the user to specify the max number of pages to scrape from the PDF and an ability to scrape a subset of random pages from the PDF. This functionality was created so that, when dealing with especially large PDF documents, features could still be generated from the pages deep in a PDF document without limiting the scrape to the first couple of pages in a PDF document. In the next stage of development, where a classifier is built to separate forms from non-forms, this functionality will be great for feature generation. \n",
    "\n",
    "See source code: https://github.com/vidal-anguiano/LSDM-Assignment-2/blob/master/pdfscrape/pdf_pipeline_ns.py\n",
    "\n",
    "\n",
    "\n",
    "### Use of Stanford CoreNLP\n",
    "In this initial iteration, I demonstrate the ability to take text and run it through Stanford CoreNLP. The outputs generated from hitting the CoreNLP server are data on the named entities in the text. For future iterations, CoreNLP will be used heavily for further feature generation to be used in classifying PDFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate connection to running Stanford CoreNLP server. The PyCoreNLP package offers a wrapper on the Stanford CoreNLP API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = pycorenlp.StanfordCoreNLP(\"http://ec2-18-234-241-82.compute-1.amazonaws.com:9000\")\n",
    "nlp = pycorenlp.StanfordCoreNLP(\"http://localhost:9000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the below mp.Queue() instances are either used in this implementation or built now for future implementation. Notes on each one are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tovisit_q = mp.Queue()  # Stores the URLs that are to be visited on a FIFO basis\n",
    "\n",
    "writeto_q = mp.Queue()  # NOT IMPLEMENTED - eventually will be used to hold data that is to later be written by\n",
    "                        # another process\n",
    "    \n",
    "faildrd_q = mp.Queue()  # Stores the URLs that are dead or were not reached\n",
    "\n",
    "pdflink_q = mp.Queue()  # Stores links to PDF files to be scraped\n",
    "\n",
    "visited_qs = mp.Queue() # Stores a Set that is shared by all Processes and used for keeping track of the pages \n",
    "visited_qs.put(set())   # that have been visited in order to prevent putting URLs into the queue when they've \n",
    "                        # already been visited.\n",
    "\n",
    "tovisit_qs = mp.Queue() # Similar to the visited_qs, this queue holds a Set of the URLs that are already in the\n",
    "tovisit_qs.put(set())   # queue and prevents them from being added again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SKIP_URL** is used to provide a set of strings that, if found in a URL, will cause the URL to be ignored and the page will not be visited.\n",
    "\n",
    "**w** instantiates a WebScrape object which takes the above mp.Queue() objects in addition to a URL so start the scrape from and a limiting domain (lmt_doma), which ignores any URL that is not in the limiting domain.\n",
    "\n",
    "**arr** and **arr2** set the arguments for the fuction that scrapes the PDFs found on the City of Chicago website. Each of the parameters for the `scrape_pdfs` function are as follows (can also be seen in function docstring:\n",
    "- pdflink_q: Described above\n",
    "- maxpages: Maximum number of pages to be scraped from any given PDF\n",
    "- base: Number of pages to be scraped from the beginning of a PDF\n",
    "- random_sample: Number of pages to be scraped randomly from the entire PDF\n",
    "- to_scrape: Number of PDF documents to scrape from those identified on the City of Chicago website\n",
    "- scrape_file: Name of file where outputs are to be stored.\n",
    "- final: Boolean to indicate which of the processes is running last so that the mp.Queue() object can be flushed before p.join() is run (will hang otherwise).\n",
    "- nlp: Connection to running Stanford CoreNLP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_URL = ['department', 'spec', 'council', 'please', '311', 'phone', 'press', 'release']\n",
    "\n",
    "w = scrape.WebScrape('http://cityofchicago.org',\n",
    "                     tovisit_q,\n",
    "                     writeto_q,\n",
    "                     faildrd_q,\n",
    "                     pdflink_q,\n",
    "                     visited_qs,\n",
    "                     tovisit_qs,\n",
    "                     lmt_doma='www.cityofchicago.org')\n",
    "\n",
    "arr = (pdflink_q, 10, 5, 5, 30, './scrape1.csv', 'temp1.pdf',False, nlp)\n",
    "arr2 = (pdflink_q, 10, 5, 5, 60, './scrape2.csv', 'temp2.pdf', True, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "There are three processes running below. One (p1) crawls the web to collect PDF links. The second (p2) and third (p3) processes scrape PDFs, hit the Stanford CoreNLP server, and write results to a CSV file. Results include a flag indicating whether the PDF is \"fillable\" or editable electronically, the number of pages in the PDF, the text scraped from the PDF, and named entity results from Core NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1 = Process(name=\"Web Crawler\",\n",
    "             target=w.scrape,\n",
    "             args=(100,SKIP_URL))\n",
    "\n",
    "p2 = Process(name=\"PDF Scraper 1\",\n",
    "             target=pdf_pipeline_ns.scrape_pdfs,\n",
    "             args=(*arr,))\n",
    "\n",
    "p3 = Process(name=\"PDF Scraper 2\",\n",
    "            target=pdf_pipeline_ns.scrape_pdfs,\n",
    "             args=(*arr2,))\n",
    "\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "p3.start()\n",
    "\n",
    "\n",
    "p1.join()\n",
    "p2.join()\n",
    "p3.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "Future iterations will more thoroughly make use of Stanford CoreNLP for feature generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
